{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 16:19:07 WARN Utils: Your hostname, martin96-TP300LA resolves to a loopback address: 127.0.1.1; using 192.168.1.109 instead (on interface wlp2s0)\n",
      "22/03/24 16:19:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/24 16:19:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pyspark\n",
    "import findspark\n",
    "import multiprocessing\n",
    "findspark.init('/home/martin96/Spark/spark-3.2.1-bin-hadoop3.2')\n",
    "s3_client = boto3.client('s3')\n",
    "sessions3 = boto3.Session()\n",
    "s3 = sessions3.resource('s3')\n",
    "my_bucket = s3.Bucket('pinterestkafkabucket')\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    # Setting where master node is located [cores for multiprocessing]\n",
    "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
    "    # Setting application name\n",
    "    .setAppName(\"TestApp\")\n",
    "    # Setting config value via string\n",
    "    .set(\"spark.eventLog.enabled\", False)\n",
    "    # Setting environment variables for executors to use\n",
    "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
    "    # Setting memory if this setting was not set previously\n",
    "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
    ")\n",
    "sessionsp = pyspark.sql.SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = sessionsp.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 16:21:10 WARN DAGScheduler: Broadcasting large task binary with size 1899.3 KiB\n",
      "22/03/24 16:21:12 WARN DAGScheduler: Broadcasting large task binary with size 1899.3 KiB\n",
      "22/03/24 16:21:13 WARN DAGScheduler: Broadcasting large task binary with size 1899.3 KiB\n",
      "22/03/24 16:21:15 WARN DAGScheduler: Broadcasting large task binary with size 1899.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|      category|index|           unique_id|               title|         description| follower_count|            tag_list|   is_image_or_video|           image_src|downloaded|       save_location|\n",
      "+--------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|        travel|10020|56cac475-b169-42c...|10 Tips for Plann...|Ten tips to keep ...|             5k|Scotland Vacation...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|           art|  365|ee28bec4-ac3b-458...|THE BEYOND T-Shir...|Lucio Fulci's hor...|             53|Creepy,Scary,Terr...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|diy-and-crafts| 2975|f51fb8c5-c56b-4d8...|Don't throw away ...|No description av...|            84k|Diy Crafts For Ho...|multi-video(story...|https://i.pinimg....|         1|Local save in /da...|\n",
      "|      vehicles|10538|5d9fa7e2-2118-444...|BC Customs (BCC) ...|By David Crane ; ...|            709|Army Vehicles,Arm...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|     education| 4249|34b433d4-2fb4-47f...|What Is a Coproli...|What Is a Coproli...|             2M|Further Education...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|  mens-fashion| 7467|21ea7e2b-9f40-403...|5 TRUQUES REAIS p...|Macho Moda: Blog ...|           620k|Best Mens Fashion...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|        travel|10029|d91ae5c9-6cb7-432...|11 Stops To Inclu...|These Alaska itin...|             9k|Alaska Travel,Tra...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|     education| 3813|49f7dfe7-2e5a-403...|Fun Interactive D...|Tired of Google C...|             9k|Google Classroom,...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|           art|  637|7a04448a-7c86-445...|How to Draw Pinoc...|How to Draw Pinoc...|             2M|Disney Drawings S...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|     christmas| 1667|a7ae2cae-c83c-4fa...|Christmas Elf Sto...|Christmas Elf Sto...|            184|Summer Christmas,...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|diy-and-crafts| 3032|208b07bc-e042-4e8...|   Puffy Ghost Craft|This ghost craft ...|            56k|Halloween Arts An...|               video|https://i.pinimg....|         1|Local save in /da...|\n",
      "|  mens-fashion| 7528|fbe53c66-3442-477...|No Title Data Ava...|No description av...|User Info Error|N,o, ,T,a,g,s, ,A...|multi-video(story...|    Image src error.|         0|Local save in /da...|\n",
      "|diy-and-crafts| 2863|9bf39437-42a6-4f0...|25 Super Fun Summ...|Keep the kids bus...|           124k|Summer Crafts For...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|       finance| 5730|1e1f0c8b-9fcf-460...|Island Oasis Coup...|Description Coupo...|              0|Grocery Items,Gro...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|       tattoos| 8731|ea760f71-febf-402...|20 Koi Fish Tatto...|Koi fish tattoos ...|           211k|Dr Tattoo,WÃ¶rter ...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|        beauty| 1313|44662045-e891-482...|Liquid Lash Exten...|Instantly create ...|            43k|N,o, ,T,a,g,s, ,A...|               video|https://i.pinimg....|         1|Local save in /da...|\n",
      "|     education| 4315|21b59ba9-829d-4c3...|Podcasts for Teac...|Podcasts for Teac...|            25k|Middle School Cla...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|       finance| 5494|8fb2af68-543b-463...|Dave Ramsey's 7 B...|If you love budge...|            26k|Financial Peace,F...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|diy-and-crafts| 2923|52fa3af5-24a4-4cc...|UFO Paper Plate C...|A fun space activ...|           192k|Paper Plate Craft...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|diy-and-crafts| 3089|88f9227e-88d0-4b1...|No Title Data Ava...|No description av...|User Info Error|N,o, ,T,a,g,s, ,A...|multi-video(story...|    Image src error.|         0|Local save in /da...|\n",
      "+--------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "#df = sessionsp.createDataFrame(pkeys.collect())\n",
    "df_list = []\n",
    "for obj in my_bucket.objects.all():\n",
    "### Read streaming body\n",
    "    obj = s3_client.get_object(Bucket='pinterestkafkabucket', Key=obj.key)\n",
    "    content = obj['Body'].read()\n",
    "    data = json.loads(content)\n",
    "    pkeys = sc.parallelize([data])\n",
    "    norm = pd.json_normalize(pkeys.collect())\n",
    "    norm_list = norm.iloc[0].tolist()\n",
    "    df = sessionsp.createDataFrame(pd.DataFrame(norm, columns=norm.keys()))\n",
    "    df_list.append(df)\n",
    "df = reduce(pyspark.sql.DataFrame.union, df_list)\n",
    "df.show()\n",
    "#df.select(\"category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index STRING :key,category STRING cf:Incoming_Data,unique_id STRING cf:Incoming_Data,title STRING cf:Incoming_Data,description STRING cf:Incoming_Data,follower_count STRING cf:Incoming_Data,tag_list STRING cf:Incoming_Data,is_image_or_video STRING cf:Incoming_Data,image_src STRING cf:Incoming_Data,downloaded STRING cf:Incoming_Data,save_location STRING cf:Incoming_Data\n"
     ]
    }
   ],
   "source": [
    "sp_table_string = \"index INTEGER :key,\"\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != \"index\":\n",
    "        if col == \"downloaded\":\n",
    "            sp_table_string += f\"{col} INTEGER Incoming_Data:{col},\"\n",
    "        else:\n",
    "            sp_table_string += f\"{col} STRING Incoming_Data:{col},\"\n",
    "\n",
    "sp_table_string = sp_table_string[:(len(sp_table_string)-1)]\n",
    "print(sp_table_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALTER TABLE pinterest_data ADD COLUMN category VARCHAR, ADD COLUMN unique_id VARCHAR, ADD COLUMN title VARCHAR, ADD COLUMN description VARCHAR, ADD COLUMN follower_count VARCHAR, ADD COLUMN tag_list VARCHAR, ADD COLUMN is_image_or_video VARCHAR, ADD COLUMN image_src VARCHAR, ADD COLUMN downloaded INTEGER, ADD COLUMN save_location VARCHAR, ADD COLUMN\n"
     ]
    }
   ],
   "source": [
    "# postgres table\n",
    "pg_table_string = \"ALTER TABLE pinterest_data ADD COLUMN \"\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != \"index\":\n",
    "        if col == \"downloaded\":\n",
    "            pg_table_string += f\"{col} INTEGER, ADD COLUMN \"\n",
    "        else:\n",
    "            pg_table_string += f\"{col} VARCHAR, ADD COLUMN \"\n",
    "\n",
    "pg_table_string = pg_table_string[:(len(pg_table_string)-1)]\n",
    "print(pg_table_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index int :key,category STRING Incoming_Data:category,unique_id STRING Incoming_Data:unique_id,title STRING Incoming_Data:title,description STRING Incoming_Data:description,follower_count STRING Incoming_Data:follower_count,tag_list STRING Incoming_Data:tag_list,is_image_or_video STRING Incoming_Data:is_image_or_video,image_src STRING Incoming_Data:image_src,downloaded int Incoming_Data:downloaded,save_location STRING Incoming_Data:save_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/10 21:08:05 WARN FileOutputCommitter: Output Path is null in setupJob()\n",
      "22/03/10 21:08:08 WARN ClientCnxn: An exception was thrown while closing send thread for session 0x17f75917dc90064.\n",
      "EndOfStreamException: Unable to read additional data from server sessionid 0x17f75917dc90064, likely server has closed socket\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n",
      "\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1275)\n",
      "22/03/10 21:08:10 WARN ClientCnxn: An exception was thrown while closing send thread for session 0x17f75917dc90075.\n",
      "EndOfStreamException: Unable to read additional data from server sessionid 0x17f75917dc90075, likely server has closed socket\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n",
      "\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1275)\n",
      "22/03/10 21:08:10 WARN ClientCnxn: An exception was thrown while closing send thread for session 0x17f75917dc90074.\n",
      "EndOfStreamException: Unable to read additional data from server sessionid 0x17f75917dc90074, likely server has closed socket\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n",
      "\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1275)\n",
      "22/03/10 21:08:12 WARN ClientCnxn: An exception was thrown while closing send thread for session 0x17f75917dc9007f.\n",
      "EndOfStreamException: Unable to read additional data from server sessionid 0x17f75917dc9007f, likely server has closed socket\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n",
      "\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1275)\n",
      "22/03/10 21:08:12 WARN ClientCnxn: An exception was thrown while closing send thread for session 0x17f75917dc9007e.\n",
      "EndOfStreamException: Unable to read additional data from server sessionid 0x17f75917dc9007e, likely server has closed socket\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n",
      "\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1275)\n",
      "22/03/10 21:08:14 WARN FileOutputCommitter: Output Path is null in commitJob()  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|      category|index|           unique_id|               title|         description|follower_count|            tag_list|   is_image_or_video|           image_src|downloaded|       save_location|\n",
      "+--------------+-----+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|        travel|10020|56cac475-b169-42c...|10 Tips for Plann...|Ten tips to keep ...|            5k|Scotland Vacation...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|           art|  365|ee28bec4-ac3b-458...|THE BEYOND T-Shir...|Lucio Fulci's hor...|            53|Creepy,Scary,Terr...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|diy-and-crafts| 2975|f51fb8c5-c56b-4d8...|Don't throw away ...|No description av...|           84k|Diy Crafts For Ho...|multi-video(story...|https://i.pinimg....|         1|Local save in /da...|\n",
      "|      vehicles|10538|5d9fa7e2-2118-444...|BC Customs (BCC) ...|By David Crane ; ...|           709|Army Vehicles,Arm...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|     education| 4249|34b433d4-2fb4-47f...|What Is a Coproli...|What Is a Coproli...|            2M|Further Education...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|  mens-fashion| 7467|21ea7e2b-9f40-403...|5 TRUQUES REAIS p...|Macho Moda: Blog ...|          620k|Best Mens Fashion...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|        travel|10029|d91ae5c9-6cb7-432...|11 Stops To Inclu...|These Alaska itin...|            9k|Alaska Travel,Tra...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|     education| 3813|49f7dfe7-2e5a-403...|Fun Interactive D...|Tired of Google C...|            9k|Google Classroom,...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|           art|  637|7a04448a-7c86-445...|How to Draw Pinoc...|How to Draw Pinoc...|            2M|Disney Drawings S...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|     christmas| 1667|a7ae2cae-c83c-4fa...|Christmas Elf Sto...|Christmas Elf Sto...|           184|Summer Christmas,...|               image|https://i.pinimg....|         1|Local save in /da...|\n",
      "|diy-and-crafts| 3032|208b07bc-e042-4e8...|   Puffy Ghost Craft|This ghost craft ...|           56k|Halloween Arts An...|               video|https://i.pinimg....|         1|Local save in /da...|\n",
      "+--------------+-----+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import multiprocessing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext, SparkContext, SparkConf\n",
    "\n",
    "findspark.init('/home/martin96/Spark/spark-3.2.1-bin-hadoop3.2')\n",
    "findspark.find()\n",
    "\n",
    "sp_table_string = \"index int :key,\"\n",
    "for col in df.columns:\n",
    "    if col != \"index\":\n",
    "        if col == \"downloaded\":\n",
    "            sp_table_string += f\"{col} int Incoming_Data:{col},\"\n",
    "        else:\n",
    "            sp_table_string += f\"{col} STRING Incoming_Data:{col},\"\n",
    "\n",
    "sp_table_string = sp_table_string[:(len(sp_table_string)-1)]\n",
    "# print(sp_table_string)\n",
    "\n",
    "def write_to_hbase():\n",
    "    conf = (SparkConf().setAppName(\"RW_from_HBase\"))\n",
    "\n",
    "    spark = SparkSession.builder.appName(\" \").config(conf=conf).getOrCreate()\n",
    "\n",
    "    # hb_df = spark.read.format(\"org.apache.hadoop.hbase.spark\").option(\"hbase.columns.mapping\",\n",
    "    #         f\"{sp_table_string}\").option(\"hbase.spark.pushdown.columnfilter\", True) \\\n",
    "    #             .option(\"hbase.table\", \"Pinterest_Data_v2\") \\\n",
    "    #             .option(\"hbase.spark.use.hbasecontext\", False) \\\n",
    "    #             .load()\n",
    "\n",
    "    df.write.format(\"org.apache.hadoop.hbase.spark\").option(\"hbase.columns.mapping\",\n",
    "            f\"{sp_table_string}\").option(\"hbase.spark.pushdown.columnfilter\", True) \\\n",
    "                .option(\"hbase.table\", \"Pinterest_Data_v2\") \\\n",
    "                .option(\"hbase.spark.use.hbasecontext\", False) \\\n",
    "                .save()\n",
    "    df.createOrReplaceTempView(\"table\")\n",
    "    # #spark.sql(\"INSERT INTO table (rowKey, f_name, l_name) VALUES ('3', 'steve','smith')\")\n",
    "    \n",
    "    df.show()\n",
    "\n",
    "write_to_hbase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index int :key,category STRING Incoming_Data:category,unique_id STRING Incoming_Data:unique_id,title STRING Incoming_Data:title,description STRING Incoming_Data:description,follower_count STRING Incoming_Data:follower_count,tag_list STRING Incoming_Data:tag_list,is_image_or_video STRING Incoming_Data:is_image_or_video,image_src STRING Incoming_Data:image_src,downloaded int Incoming_Data:downloaded,save_location STRING Incoming_Data:save_location\n",
      "+-----+\n",
      "|index|\n",
      "+-----+\n",
      "|  365|\n",
      "|  637|\n",
      "| 1667|\n",
      "| 2975|\n",
      "| 3032|\n",
      "| 3813|\n",
      "| 4249|\n",
      "| 7467|\n",
      "|10020|\n",
      "|10029|\n",
      "|10538|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import multiprocessing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext, SparkContext, SparkConf\n",
    "\n",
    "findspark.init('/home/martin96/Spark/spark-3.2.1-bin-hadoop3.2')\n",
    "findspark.find()\n",
    "spark = SparkSession.builder.appName(\" \").config(conf=conf).getOrCreate()\n",
    "hb_df = spark.read.format(\"org.apache.hadoop.hbase.spark\").option(\"hbase.columns.mapping\",\n",
    "        f\"{sp_table_string}\").option(\"hbase.spark.pushdown.columnfilter\", True) \\\n",
    "            .option(\"hbase.table\", \"Pinterest_Data_v2\") \\\n",
    "            .option(\"hbase.spark.use.hbasecontext\", False) \\\n",
    "            .load()\n",
    "hb_df.select('index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/10 21:09:10 ERROR Executor: Exception in task 0.0 in stage 44.0 (TID 366)\n",
      "org.apache.hadoop.hbase.DoNotRetryIOException: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n",
      "\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n",
      "\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n",
      "\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n",
      "\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:348)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n",
      "\t... 7 more\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:99)\n",
      "\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:89)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:284)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:271)\n",
      "\tat org.apache.hadoop.hbase.client.RegionServerCallable.call(RegionServerCallable.java:129)\n",
      "\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:108)\n",
      "\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\n",
      "\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\n",
      "\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:397)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.TableResource.$anonfun$get$1(HBaseResources.scala:118)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException(HBaseResources.scala:81)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException$(HBaseResources.scala:77)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.TableResource.releaseOnException(HBaseResources.scala:93)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.TableResource.get(HBaseResources.scala:118)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.$anonfun$buildGets$1(HBaseTableScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.DoNotRetryIOException): org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n",
      "\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n",
      "\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n",
      "\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n",
      "\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:348)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n",
      "\t... 7 more\n",
      "\n",
      "\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:380)\n",
      "\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89)\n",
      "\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414)\n",
      "\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410)\n",
      "\tat org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:118)\n",
      "\tat org.apache.hadoop.hbase.ipc.Call.setException(Call.java:133)\n",
      "\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.readResponse(NettyRpcDuplexHandler.java:162)\n",
      "\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.channelRead(NettyRpcDuplexHandler.java:192)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "22/03/10 21:09:10 WARN TaskSetManager: Lost task 0.0 in stage 44.0 (TID 366) (martin96-TP300LA.lan executor driver): org.apache.hadoop.hbase.DoNotRetryIOException: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n",
      "\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n",
      "\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n",
      "\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n",
      "\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:348)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n",
      "\t... 7 more\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:99)\n",
      "\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:89)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:284)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:271)\n",
      "\tat org.apache.hadoop.hbase.client.RegionServerCallable.call(RegionServerCallable.java:129)\n",
      "\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:108)\n",
      "\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\n",
      "\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\n",
      "\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:397)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.TableResource.$anonfun$get$1(HBaseResources.scala:118)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException(HBaseResources.scala:81)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException$(HBaseResources.scala:77)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.TableResource.releaseOnException(HBaseResources.scala:93)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.TableResource.get(HBaseResources.scala:118)\n",
      "\tat org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.$anonfun$buildGets$1(HBaseTableScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.DoNotRetryIOException): org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n",
      "\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n",
      "\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n",
      "\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n",
      "\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n",
      "\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:348)\n",
      "\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n",
      "\t... 7 more\n",
      "\n",
      "\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:380)\n",
      "\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89)\n",
      "\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414)\n",
      "\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410)\n",
      "\tat org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:118)\n",
      "\tat org.apache.hadoop.hbase.ipc.Call.setException(Call.java:133)\n",
      "\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.readResponse(NettyRpcDuplexHandler.java:162)\n",
      "\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.channelRead(NettyRpcDuplexHandler.java:192)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      "22/03/10 21:09:10 ERROR TaskSetManager: Task 0 in stage 44.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o722.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 366) (martin96-TP300LA.lan executor driver): org.apache.hadoop.hbase.DoNotRetryIOException: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n\t... 7 more\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:99)\n\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:89)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:284)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:271)\n\tat org.apache.hadoop.hbase.client.RegionServerCallable.call(RegionServerCallable.java:129)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:108)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:397)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.$anonfun$get$1(HBaseResources.scala:118)\n\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException(HBaseResources.scala:81)\n\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException$(HBaseResources.scala:77)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.releaseOnException(HBaseResources.scala:93)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.get(HBaseResources.scala:118)\n\tat org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.$anonfun$buildGets$1(HBaseTableScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.DoNotRetryIOException): org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n\t... 7 more\n\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:380)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410)\n\tat org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:118)\n\tat org.apache.hadoop.hbase.ipc.Call.setException(Call.java:133)\n\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.readResponse(NettyRpcDuplexHandler.java:162)\n\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.channelRead(NettyRpcDuplexHandler.java:192)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat sun.reflect.GeneratedMethodAccessor83.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.hbase.DoNotRetryIOException: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n\t... 7 more\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:99)\n\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:89)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:284)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:271)\n\tat org.apache.hadoop.hbase.client.RegionServerCallable.call(RegionServerCallable.java:129)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:108)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:397)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.$anonfun$get$1(HBaseResources.scala:118)\n\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException(HBaseResources.scala:81)\n\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException$(HBaseResources.scala:77)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.releaseOnException(HBaseResources.scala:93)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.get(HBaseResources.scala:118)\n\tat org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.$anonfun$buildGets$1(HBaseTableScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.DoNotRetryIOException): org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n\t... 7 more\n\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:380)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410)\n\tat org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:118)\n\tat org.apache.hadoop.hbase.ipc.Call.setException(Call.java:133)\n\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.readResponse(NettyRpcDuplexHandler.java:162)\n\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.channelRead(NettyRpcDuplexHandler.java:192)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4167/1411557000.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhb_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM table WHERE index=365;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmy_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/AICORE_Course_Env/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/AICORE_Course_Env/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/AICORE_Course_Env/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/AICORE_Course_Env/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o722.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 366) (martin96-TP300LA.lan executor driver): org.apache.hadoop.hbase.DoNotRetryIOException: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n\t... 7 more\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:99)\n\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:89)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:284)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:271)\n\tat org.apache.hadoop.hbase.client.RegionServerCallable.call(RegionServerCallable.java:129)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:108)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:397)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.$anonfun$get$1(HBaseResources.scala:118)\n\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException(HBaseResources.scala:81)\n\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException$(HBaseResources.scala:77)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.releaseOnException(HBaseResources.scala:93)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.get(HBaseResources.scala:118)\n\tat org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.$anonfun$buildGets$1(HBaseTableScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.DoNotRetryIOException): org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n\t... 7 more\n\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:380)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410)\n\tat org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:118)\n\tat org.apache.hadoop.hbase.ipc.Call.setException(Call.java:133)\n\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.readResponse(NettyRpcDuplexHandler.java:162)\n\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.channelRead(NettyRpcDuplexHandler.java:192)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat sun.reflect.GeneratedMethodAccessor83.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.hbase.DoNotRetryIOException: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n\t... 7 more\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:99)\n\tat org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:89)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:284)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:271)\n\tat org.apache.hadoop.hbase.client.RegionServerCallable.call(RegionServerCallable.java:129)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:108)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\n\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:397)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.$anonfun$get$1(HBaseResources.scala:118)\n\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException(HBaseResources.scala:81)\n\tat org.apache.hadoop.hbase.spark.datasources.ReferencedResource.releaseOnException$(HBaseResources.scala:77)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.releaseOnException(HBaseResources.scala:93)\n\tat org.apache.hadoop.hbase.spark.datasources.TableResource.get(HBaseResources.scala:118)\n\tat org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.$anonfun$buildGets$1(HBaseTableScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.DoNotRetryIOException): org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1724)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toGet(ProtobufUtil.java:556)\n\tat org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2363)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:36800)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2423)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:188)\n\tat org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:150)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.toFilter(ProtobufUtil.java:1714)\n\t... 7 more\n\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:380)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:414)\n\tat org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410)\n\tat org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:118)\n\tat org.apache.hadoop.hbase.ipc.Call.setException(Call.java:133)\n\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.readResponse(NettyRpcDuplexHandler.java:162)\n\tat org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.channelRead(NettyRpcDuplexHandler.java:192)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n\tat org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "hb_df.createOrReplaceTempView(\"table\")\n",
    "my_row = spark.sql(\"SELECT * FROM table WHERE index=365;\")\n",
    "my_row.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b957f52f813f83e0b8321e25f7fdd54a5fc4f710366bb94dfdef51730d1694ed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('AICORE_Course_Env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
